{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title:       train.py\n",
    "Description: Collection of functions to train the model.\n",
    "Author:      Kunyu He, CAPP'20\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score)\n",
    "\n",
    "from viz import plot_predicted_scores, plot_precision_recall\n",
    "\n",
    "\n",
    "INPUT_DIR = \"../processed_data/\"\n",
    "OUTPUT_DIR = \"../log/\"\n",
    "\n",
    "MODEL_NAMES = [\"KNN\", \"Logistic Regression\", \"Decision Tree\", \"Linear SVM\",\n",
    "               \"Random Forest\"]\n",
    "MODELS = [KNeighborsClassifier, LogisticRegression, DecisionTreeClassifier,\n",
    "          LinearSVC, RandomForestClassifier]\n",
    "\n",
    "METRICS_NAMES = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC ROC Score\"]\n",
    "METRICS = [accuracy_score, precision_score, recall_score, f1_score, roc_auc_score]\n",
    "\n",
    "THRESHOLDS = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "SEED = 123\n",
    "\n",
    "GRID_SEARCH_PARAMS = {\"KNN\": {\n",
    "                              'n_neighbors': list(range(50, 110, 20)),\n",
    "                              'weights': [\"uniform\", \"distance\"],\n",
    "                              'metric': [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "                              },\n",
    "\n",
    "                      \"Logistic Regression\": {\n",
    "                                              'penalty': ['l1', 'l2'],\n",
    "                                              'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                                              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "                                              },\n",
    "\n",
    "                      \"Decision Tree\": {\n",
    "                            'criterion': [\"entropy\", \"gini\"],\n",
    "                            'min_samples_split': list(np.arange(0.02, 0.05, 0.01)),\n",
    "                            'max_depth': list(range(4, 11)),\n",
    "                            'max_features': list(range(4, 15, 2))\n",
    "                            },\n",
    "\n",
    "                      \"Linear SVM\": {\n",
    "                                     'penalty': ['l1', 'l2'],\n",
    "                                     'C': [0.001, 0.01, 0.1, 1, 10]\n",
    "                                     },\n",
    "\n",
    "                      \"Random Forest\": {\n",
    "                            'min_samples_split': list(np.arange(0.01, 0.06, 0.01)),\n",
    "                            'max_depth': list(range(4, 11)),\n",
    "                            'max_features': list(range(4, 15, 2))\n",
    "                            }\n",
    "                      }\n",
    "\n",
    "DEFAULT_ARGS = {\"KNN\": {'n_jobs': -1},\n",
    "                \"Logistic Regression\": {'random_state': SEED},\n",
    "                \"Decision Tree\": {'random_state': SEED},\n",
    "                \"Linear SVM\": {'random_state': SEED, 'tol': 1e-3},\n",
    "                \"Random Forest\": {'n_estimators': 300, 'random_state': SEED,\n",
    "                                  'oob_score': True}}\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "def ask():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"Up till now we support:\\n\")\n",
    "    for i in range(len(MODEL_NAMES)):\n",
    "        print(\"{}. {}\".format(i, MODEL_NAMES[i]))\n",
    "    model_index = int(input((\"We use default Decision Tree as the benchmark.\\n\"\n",
    "                             \"Please input a classifier index:\\n\")))\n",
    "\n",
    "    print((\"Up till now we use the following metrics to evaluate the\"\n",
    "           \" fitted classifiers on the validation and test set.\\n\"))\n",
    "    for i in range(len(METRICS)):\n",
    "        print(\"{}. {}\".format(i, METRICS_NAMES[i].title()))\n",
    "    metric_index = int(input(\"Please input a metrics index:\\n\"))\n",
    "\n",
    "    return model_index, metric_index\n",
    "\n",
    "\n",
    "def load_features(dir_path=INPUT_DIR, test=True):\n",
    "    \"\"\"\n",
    "    Load pre-processed feature matrices.\n",
    "\n",
    "    \"\"\"\n",
    "    Xs = np.load(dir_path + 'X.npz')\n",
    "    ys = np.load(dir_path + 'y.npz')\n",
    "\n",
    "    if not test:\n",
    "        X_train = Xs['train']\n",
    "        y_train = ys['train']\n",
    "        return X_train, y_train\n",
    "\n",
    "    X_train, X_test = Xs['train'], Xs['test']\n",
    "    y_train, y_test = ys['train'], ys['test']\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_benchmark(data, metric_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    benchmark = DecisionTreeClassifier(**DEFAULT_ARGS[\"Decision Tree\"])\n",
    "    benchmark.fit(X_train, y_train)\n",
    "    predicted_probs = benchmark.predict_proba(X_test)[:, 1]\n",
    "    benchmark_score = METRICS[metric_index](y_test, benchmark.predict(X_test))\n",
    "\n",
    "    print(\"{} of the benchmark default decision tree model is {:.4f}.\\n\".\\\n",
    "          format(METRICS_NAMES[metric_index], round(benchmark_score, 4)))\n",
    "\n",
    "    return benchmark_score\n",
    "\n",
    "\n",
    "def clf_predict_proba(clf, X_test):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        predicted_prob = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        prob = clf.decision_function(X_test)\n",
    "        predicted_prob = (prob - prob.min()) / (prob.max() - prob.min())\n",
    "\n",
    "    return predicted_prob\n",
    "\n",
    "\n",
    "def cross_validation(clf, skf, data, metric_index, threshold):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_train, y_train = data\n",
    "    predicted_probs, scores = [], []\n",
    "\n",
    "    for train, validation in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train], X_train[validation]\n",
    "        y_tr, y_val = y_train[train], y_train[validation]\n",
    "\n",
    "        try:\n",
    "            clf.fit(X_tr, y_tr)\n",
    "        except:\n",
    "            return None, 0.0\n",
    "        predicted_prob = clf_predict_proba(clf, X_val)\n",
    "        predicted_labels = np.where(predicted_prob > threshold, 1, 0)\n",
    "\n",
    "        predicted_probs.append(predicted_prob)\n",
    "        scores.append(METRICS[metric_index](y_val, predicted_labels))\n",
    "\n",
    "    return list(itertools.chain(*predicted_probs)), np.array(scores).mean()\n",
    "\n",
    "\n",
    "def find_best_threshold(model_index, metric_index, train_data,\n",
    "                        verbose=False, plot=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model_name = MODEL_NAMES[model_index]\n",
    "    metric_name = METRICS_NAMES[metric_index]\n",
    "    default_args = DEFAULT_ARGS[model_name]\n",
    "\n",
    "    clf = MODELS[model_index](**default_args)\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=SEED)\n",
    "\n",
    "    if plot:\n",
    "        default_probs, _ = cross_validation(clf, skf, train_data, metric_index, 0.5)\n",
    "        plot_predicted_scores(default_probs)\n",
    "\n",
    "    best_score, best_threshold = 0, None\n",
    "    print(\"Default {}. Search Starts:\".format(model_name))\n",
    "    for threshold in THRESHOLDS:\n",
    "        _, score = cross_validation(clf, skf, train_data, metric_index, threshold)\n",
    "        if verbose:\n",
    "            print(\"\\t(Threshold: {}) the cross-validation {} is {:.4f}\".\\\n",
    "                  format(threshold, metric_name, score))\n",
    "        if score > best_score:\n",
    "            best_score, best_threshold = score, threshold\n",
    "\n",
    "    print(\"Search Finished: The best threshold to use is {:.4f}.\\n\".format(best_threshold))\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def tune(model_index, metric_index, train_data, best_threshold,\n",
    "         n_folds=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Use grid search and cross validation to find the best set of hyper-\n",
    "    parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    model_name = MODEL_NAMES[model_index]\n",
    "    metric_name = METRICS_NAMES[metric_index]\n",
    "    params_grid = GRID_SEARCH_PARAMS[model_name]\n",
    "    default_args = DEFAULT_ARGS[model_name]\n",
    "\n",
    "    best_score, best_grid = 0, None\n",
    "    params = params_grid.keys()\n",
    "    skf = StratifiedKFold(n_splits=n_folds, random_state=SEED)\n",
    "\n",
    "    print(\"{} with Decision Threshold {}. Search Starts:\".format(model_name,\n",
    "                                                                 best_threshold))\n",
    "    for grid in itertools.product(*(params_grid[param] for param in params)):\n",
    "        args = dict(zip(params, grid))\n",
    "        clf = MODELS[model_index](**default_args, **args)\n",
    "        _, grid_score = cross_validation(clf, skf, train_data, metric_index, best_threshold)\n",
    "\n",
    "        if grid_score > best_score:\n",
    "            best_score, best_grid = grid_score, args\n",
    "        if verbose:\n",
    "            print(\"\\t(Parameters: {}), cross-validation {} of {:.4f}\".format(args, metric_name,\n",
    "                                                                             grid_score))\n",
    "\n",
    "    print(\"Search Finished: The best parameters to use is {}\\n\".format(best_grid))\n",
    "    return best_grid, best_score\n",
    "\n",
    "\n",
    "def evaluate_best_model(model_index, metric_index, best_threshold, best_grid, data,\n",
    "                        plot=False, verbose=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    model_name = MODEL_NAMES[model_index]\n",
    "    metric_name = METRICS_NAMES[metric_index]\n",
    "    default_args = DEFAULT_ARGS[model_name]\n",
    "\n",
    "    clf = MODELS[model_index](**default_args, **best_grid)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted_prob = clf_predict_proba(clf, X_test)\n",
    "    predicted_labels = np.where(predicted_prob > best_threshold, 1, 0)\n",
    "    test_score = METRICS[metric_index](y_test, predicted_labels)\n",
    "    print((\"Our {} classifier reached a(n) {} of {:.4f} with a decision\"\n",
    "           \" threshold of {} on the test set.\\n\").format(model_name, metric_name,\n",
    "                                                       test_score, best_threshold))\n",
    "\n",
    "    if plot:\n",
    "        pos = np.count_nonzero(np.append(y_train, y_test))\n",
    "        prop = pos / (len(y_test) + len(y_train))\n",
    "        plot_precision_recall(predicted_prob, y_test, prop)\n",
    "\n",
    "    return test_score\n",
    "\n",
    "\n",
    "def train_evaluate(model_index, metric_index, data, train_data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    metric_name = METRICS[metric_index]\n",
    "    model_name = MODEL_NAMES[model_index]\n",
    "\n",
    "    benchmark_score = build_benchmark(data, metric_index)\n",
    "    best_threshold = find_best_threshold(model_index, metric_index,\n",
    "                                         train_data, verbose=True, plot=True)\n",
    "    best_grid, _ = tune(model_index, metric_index, train_data, best_threshold,\n",
    "                        verbose=True)\n",
    "    test_score = evaluate_best_model(model_index, metric_index, best_threshold,\n",
    "                                     best_grid, data, plot=False, verbose=True)\n",
    "\n",
    "    diff = test_score - benchmark_score\n",
    "    print((\"{} of the tuned {} is {}, {} {} than the benchmark.\\n\"\n",
    "           \"**-------------------------------------------------------------**\\n\\n\").\\\n",
    "           format(metric_name, model_name, round(test_score.mean(), 4), diff,\n",
    "                  ['higher', 'lower'][int(diff <= 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train, X_test, y_train, y_test = load_features()\n",
    "    data = [X_train, X_test, y_train, y_test]\n",
    "    train_data = [X_train, y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index, metric_index = 3, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate(model_index, metric_index, data, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
